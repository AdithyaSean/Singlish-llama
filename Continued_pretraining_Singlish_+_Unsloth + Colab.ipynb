{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdithyaSean/Singlish-llama/blob/main/Continued_pretraining_Singlish_%2B_Unsloth%20%2B%20Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "### Hugging Face Hub Login\n",
        "\n",
        "In this cell, we log in to the Hugging Face Hub using a personal access token. This allows us to access and manage our models and datasets on the Hugging Face platform. The `login` function from the `huggingface_hub` module is used for this purpose. Make sure to replace `hf_xxx` with your actual Hugging Face token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJPlDNeroilx"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "token = userdata.get('adithyasean')\n",
        "login(token, add_to_git_credential = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6h_ds7sWxJ8"
      },
      "source": [
        "### Why We Choose to Use Unsloth\n",
        "\n",
        "Unsloth is a powerful library designed to optimize hardware performance and reduce the hardware requirements for running large language models (LLMs). By leveraging Unsloth, we can efficiently manage memory and computational resources, enabling us to run complex models on less powerful hardware. This is particularly beneficial for fine-tuning and inference tasks, where resource constraints can be a significant bottleneck.\n",
        "\n",
        "### Explanation\n",
        "\n",
        "In the following cell, we install Unsloth along with other essential packages such as Xformers (Flash Attention), TRL, PEFT, Accelerate, BitsAndBytes, and Triton. These packages are crucial for optimizing the performance of our LLM model. We also check the Torch version to determine the appropriate version of Xformers to install, ensuring compatibility and optimal performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, TextStreamer\n",
        "from unsloth import is_bfloat16_supported, UnslothTrainer, UnslothTrainingArguments, FastLanguageModel\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gO0S3GtZWxJ9"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/Llama-3.2-3B\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkDaptn8WxJ9"
      },
      "source": [
        "<a></a>\n",
        "### Data Preparation\n",
        "\n",
        "We have collected two large datasets from Hugging Face, which contain close to 90 million Sinhala examples, and a Wikipedia subset of Sinhala, which contains close to 25k Sinhala title and article-based examples. Additionally, we have another large dataset containing English to Sinhala translations by a Hugging Face user and the Sinhala translation of the Alpaca dataset by another Hugging Face user.\n",
        "\n",
        "All the datasets were transliterated and processed by custom Python scripts - [GitHub Repository](https://github.com/AdithyaSean/Singlish-llama).\n",
        "\n",
        "Datasets by trained order:\n",
        "\n",
        "**Original Datasets:**\n",
        "- [Sinhala 30M](https://huggingface.co/datasets/9wimu9/sinhala_30m)\n",
        "- [Sinhala Dataset 59M](https://huggingface.co/datasets/9wimu9/sinhala_dataset_59m)\n",
        "- [Wikipedia (Sinhala subset)](https://huggingface.co/datasets/wikimedia/wikipedia)\n",
        "- [English-Sinhala Translated](https://huggingface.co/datasets/Udith-Sandaruwan/english-sinhala-translated)\n",
        "- [Alpaca-Sinhala](https://huggingface.co/datasets/sahanruwantha/alpaca-sinhala)\n",
        "- [sinhala-instruction-finetune-large](https://huggingface.co/datasets/ihalage/sinhala-instruction-finetune-large)\n",
        "\n",
        "**Transliterated and Processed Datasets:**\n",
        "- [Singlish 80M](https://huggingface.co/datasets/adithyasean/singlish_30m)\n",
        "- [Singlish Wikipedia](https://huggingface.co/datasets/adithyasean/singlish-wikipedia)\n",
        "- [English-Singlish](https://huggingface.co/datasets/adithyasean/english-singlish)\n",
        "- [Alpaca-Singlish](https://huggingface.co/datasets/adithyasean/alpaca-singlish)\n",
        "- [singlish-instruction-finetune](https://huggingface.co/datasets/adithyasean/singlish-instruction-finetune)\n",
        "\n",
        "These datasets are crucial for developing a new domain-specific language skill for the LLM model without affecting its current abilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flw0XLBqWxJ-"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Continued Pretraining\n",
        "Using Unsloth's `UnslothTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HKJdpldWxJ-"
      },
      "source": [
        "### First LoRA Adapter\n",
        "\n",
        "In this section, we will perform the initial pretraining of the model using the collected datasets. This step is crucial to develop a new domain-specific language skill for the LLM model without affecting its current abilities. We will utilize Parameter Efficient Fine Tuning (PEFT) methods, specifically LoRA techniques, to achieve this goal. By adding LoRA adapters, we only need to update a small percentage of all parameters, making the process efficient and effective."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nDAAFSbWxJ-"
      },
      "source": [
        "### Target Modules for LoRA Adapters\n",
        "\n",
        "In the context of adding LoRA adapters to our model, the target modules are specific parts of the model's architecture where the low-rank adaptation will be applied. These modules are chosen based on their significance in the model's computation and their potential impact on performance when fine-tuned. Here are the target modules we included and the reasons for their inclusion:\n",
        "\n",
        "- **q_proj, k_proj, v_proj, o_proj**: These are the query, key, value, and output projection layers in the attention mechanism. Fine-tuning these layers helps in adapting the attention mechanism to new tasks or domains.\n",
        "- **gate_proj, up_proj, down_proj**: These layers are part of the feed-forward neural network within the transformer architecture. Fine-tuning these layers allows the model to better capture and adapt to new patterns in the data.\n",
        "- **embed_tokens**: This module is responsible for converting input tokens into embeddings. Fine-tuning this layer helps the model to learn new token representations, which is crucial for handling out-of-distribution data.\n",
        "- **lm_head**: The language modeling head is responsible for generating the final output tokens. Fine-tuning this layer ensures that the model can produce accurate and relevant outputs for the new tasks.\n",
        "\n",
        "By including all these target modules, we ensure that the model can efficiently adapt to new tasks and data distributions while maintaining its overall performance. This comprehensive approach allows us to update only a small subset of the model's parameters, making the fine-tuning process more efficient and effective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvI4DR7gWxJ_"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZWjTiw_WxJ_"
      },
      "source": [
        "### Text datasets 100 000 examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EsKrpkza3VB3"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"adithyasean/singlish_80m\", split='train[:8000]')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_text_function(examples):\n",
        "  texts = examples[\"text\"]\n",
        "  outputs = []\n",
        "  for text in texts:\n",
        "      # Must add EOS_TOKEN, otherwise the generation will go on forever!\n",
        "      text = format(text) + EOS_TOKEN\n",
        "      outputs.append(text)\n",
        "  return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "dataset = dataset.map(formatting_text_function, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0])"
      ],
      "metadata": {
        "id": "4P8g9FBFdGAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkAzbZG9WxJ_"
      },
      "outputs": [],
      "source": [
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 8,\n",
        "\n",
        "        # Use warmup_ratio and num_train_epochs for longer runs!\n",
        "        # max_steps = 120,\n",
        "        # warmup_steps = 10,\n",
        "        warmup_ratio = 0.1,\n",
        "        num_train_epochs = 1,\n",
        "\n",
        "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
        "        learning_rate = 5e-5,\n",
        "        embedding_learning_rate = 1e-5,\n",
        "\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9sWJEOtWxJ_"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHu0D5fCWxJ_"
      },
      "source": [
        "### Wikipedia Sinhala subset 23 000 examples\n",
        "\n",
        "The title-text dataset is chosen for training the same LoRA (Low-Rank Adaptation) adapter because it offers several advantages:\n",
        "\n",
        "1. **Concise Summaries**: The dataset provides concise summaries of content, helping the model to understand and generate relevant summaries.\n",
        "2. **Title-Content Relationship**: It helps the model learn how to relate titles to their corresponding content, which is crucial for generating coherent and informative summaries.\n",
        "3. **Skill Development**: It aids the model in developing the ability to generate concise summaries, a valuable skill for various natural language processing tasks.\n",
        "\n",
        "These characteristics make the title-text dataset an excellent choice for training a LoRA adapter aimed at improving summarization capabilities.\n",
        "\n",
        "The title-text dataset is a crucial component in our training process for several reasons. First, the title-text dataset provides a concise summary of the content, which can help the model understand the main points of the text and generate more relevant summaries. Second, the title-text dataset can help the model learn how to relate the title to the content, which is essential for generating coherent and informative summaries. Finally, the title-text dataset can help the model learn how to generate concise summaries, which is a valuable skill for many natural language processing tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljQ8QUvKWxJ_"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"adithyasean/singlish-wikipedia\", split=\"train[:8000]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwBQ2neppAco"
      },
      "outputs": [],
      "source": [
        "wikipedia_prompt = \"\"\"Wikipedia Article\n",
        "### Title: {}\n",
        "\n",
        "### Article:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    titles = examples[\"title\"]\n",
        "    texts  = examples[\"text\"]\n",
        "    outputs = []\n",
        "    for title, text in zip(titles, texts):\n",
        "        # Must add EOS_TOKEN, otherwise the generation will go on forever!\n",
        "        text = wikipedia_prompt.format(title, text) + EOS_TOKEN\n",
        "        outputs.append(text)\n",
        "    return { \"text\" : outputs, }\n",
        "pass\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7ByUWfQWxJ_"
      },
      "outputs": [],
      "source": [
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nd_cBXC8WxKA"
      },
      "outputs": [],
      "source": [
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 8,\n",
        "\n",
        "        # Use warmup_ratio and num_train_epochs for longer runs!\n",
        "        # max_steps = 120,\n",
        "        # warmup_steps = 10,\n",
        "        warmup_ratio = 0.1,\n",
        "        num_train_epochs = 1,\n",
        "\n",
        "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
        "        learning_rate = 5e-5,\n",
        "        embedding_learning_rate = 1e-5,\n",
        "\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HEZe9b56WxKA"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evVOQWtQWxKA"
      },
      "source": [
        "### Second LoRA Adapter\n",
        "The concept of using multiple LoRA (Low-Rank Adaptation) adapters in machine learning, particularly in natural language processing (NLP), is often driven by the need to specialize models for different tasks or domains.\n",
        "\n",
        "Why Use a Second LoRA Adapter?\n",
        "\n",
        "Using multiple LoRA adapters allows for task-specific specialization, improved performance, and better handling of diverse linguistic and cultural nuances. It ensures that the model remains versatile and robust across different domains and tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh4I34ODWxKA"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",\n",
        "                      \"embed_tokens\", \"lm_head\",], # Add for continual pretraining\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DQe_5u-WxKA"
      },
      "source": [
        "### English to Singlish dataset 10 000 examples\n",
        "Training on English-Singlish data is crucial for:\n",
        "\n",
        "1. **Cross-Language Sharing**: Bridges English and Singlish, enhancing translation.\n",
        "2. **Knowledge Expansion**: Enriches the model with diverse information.\n",
        "3. **Language Understanding**: Improves grasp of linguistic nuances.\n",
        "4. **Model Performance**: Enhances generalization on unseen data.\n",
        "5. **Cultural Sensitivity**: Promotes inclusivity and caters to diverse needs.\n",
        "\n",
        "This results in a robust, versatile, and culturally aware language model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9JoCjDrWxKA"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"adithyasean/english-singlish\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ZXKy15pWxKA"
      },
      "outputs": [],
      "source": [
        "translation = \"\"\"Translation\n",
        "### English:\n",
        "{}\n",
        "\n",
        "### Singlish:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(examples):\n",
        "    English = examples[\"English\"]\n",
        "    Singlish  = examples[\"Singlish\"]\n",
        "    outputs = []\n",
        "    for English, Singlish in zip(English, Singlish):\n",
        "        # Must add EOS_TOKEN, otherwise the generation will go on forever!\n",
        "        text = translation.format(English, Singlish) + EOS_TOKEN\n",
        "        outputs.append(text)\n",
        "    return { \"text\" : outputs, }\n",
        "pass\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P41drOk5WxKA"
      },
      "outputs": [],
      "source": [
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 8,\n",
        "\n",
        "        # Use warmup_ratio and num_train_epochs for longer runs!\n",
        "        max_steps = 120,\n",
        "        warmup_steps = 10,\n",
        "        # warmup_ratio = 0.1,\n",
        "        # num_train_epochs = 1,\n",
        "\n",
        "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
        "        learning_rate = 5e-5,\n",
        "        embedding_learning_rate = 1e-5,\n",
        "\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KvEzr6ZPWxKA"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF-r9iufWxKA"
      },
      "source": [
        "### Instruction Finetuning\n",
        "\n",
        "Instruction fine-tuning is a crucial step in adapting our language model to follow specific instructions and generate desired outputs. By fine-tuning the model on a dataset that includes various instructions and corresponding responses, we can enhance the model's ability to understand and execute complex tasks.\n",
        "\n",
        "In this notebook, we utilize the Singlish transliteration of the Sinhala Alpaca dataset for instruction fine-tuning. This dataset contains a diverse set of instructions and responses, which helps in training the model to handle a wide range of queries effectively.\n",
        "\n",
        "The fine-tuning process involves the following steps:\n",
        "\n",
        "1. **Data Preparation**: We format the dataset to include instructions and responses, ensuring that the model can learn the relationship between them.\n",
        "2. **Model Training**: Using the `UnslothTrainer`, we fine-tune the model on the prepared dataset. This involves setting appropriate training parameters such as batch size, learning rate, and number of epochs.\n",
        "3. **Evaluation**: After training, we evaluate the model's performance to ensure it has learned to follow instructions accurately.\n",
        "\n",
        "By completing this fine-tuning process, we aim to create a robust and versatile language model capable of understanding and executing a wide range of instructions in Singlish."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TvS4TJOWxKA"
      },
      "source": [
        "### Third LoRA Adapter\n",
        "\n",
        "In this section, we introduce the third LoRA (Low-Rank Adaptation) adapter, specifically designed for instruction fine-tuning. The use of multiple LoRA adapters allows us to specialize the model for different tasks or domains, enhancing its versatility and performance.\n",
        "\n",
        "**Why Use a Third LoRA Adapter for Instruction Fine-Tuning?**\n",
        "\n",
        "1. **Task Specialization**: By adding a third LoRA adapter, we can fine-tune the model to better understand and execute specific instructions, improving its ability to handle complex tasks.\n",
        "2. **Enhanced Performance**: The additional adapter helps in refining the model's responses, making them more accurate and contextually relevant.\n",
        "3. **Diverse Instruction Handling**: With multiple adapters, the model can better manage a wide range of instructions, ensuring robust performance across different scenarios.\n",
        "4. **Efficient Fine-Tuning**: LoRA adapters allow us to update only a small subset of the model's parameters, making the fine-tuning process more efficient and less resource-intensive.\n",
        "\n",
        "By leveraging the third LoRA adapter, we aim to create a highly specialized and efficient model capable of understanding and executing a diverse set of instructions with high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iedouDKPWxKA"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 128,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha = 32,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = True,\n",
        "    loftq_config = None,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMUOZk_1WxKB"
      },
      "source": [
        "### Alpaca Dataset Sinhala\n",
        "\n",
        "The Alpaca dataset in Sinhala is a valuable resource for instruction fine-tuning. This dataset contains a diverse set of instructions and corresponding responses, which are essential for training language models to understand and execute complex tasks. By fine-tuning our model on this dataset, we aim to enhance its ability to follow specific instructions and generate accurate and relevant outputs.\n",
        "\n",
        "**Why Use the Alpaca Dataset for Instruction Fine-Tuning?**\n",
        "\n",
        "1. **Diverse Instructions**: The dataset includes a wide range of instructions, helping the model to generalize across different types of queries.\n",
        "2. **Cultural Relevance**: Being in Sinhala, it ensures that the model can handle instructions and generate responses in a culturally and linguistically appropriate manner.\n",
        "3. **Improved Performance**: Fine-tuning on this dataset helps in improving the model's performance on instruction-based tasks, making it more versatile and effective.\n",
        "4. **Enhanced Understanding**: The dataset aids in developing the model's understanding of the relationship between instructions and responses, which is crucial for generating coherent and contextually accurate outputs.\n",
        "\n",
        "By leveraging the Alpaca dataset in Sinhala, we can create a robust and culturally aware language model capable of handling a wide range of instruction-based tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fry1xDtgWxKB"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"adithyasean/alpaca-singlish\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2tv8AEhPTu6"
      },
      "outputs": [],
      "source": [
        "alpaca_prompt = \"\"\"\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Response:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "def formatting_prompts_func(conversations):\n",
        "    texts = []\n",
        "    instructions = conversations[\"instruction\"]\n",
        "    inputs = conversations[\"prompt\"]\n",
        "    outputs = conversations[\"response\"]\n",
        "    for instruction, prompt, response in zip(instructions, inputs, outputs):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = alpaca_prompt.format(instruction, prompt, response) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "pass\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve6ofYueK8Ej"
      },
      "outputs": [],
      "source": [
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PtNkB9hDWxKB"
      },
      "outputs": [],
      "source": [
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 8,\n",
        "\n",
        "        # Use warmup_ratio and num_train_epochs for longer runs!\n",
        "        max_steps = 120,\n",
        "        warmup_steps = 10,\n",
        "        # warmup_ratio = 0.1,\n",
        "        # num_train_epochs = 1,\n",
        "\n",
        "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
        "        learning_rate = 5e-5,\n",
        "        embedding_learning_rate = 1e-5,\n",
        "\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.00,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_PBYjrLWxKE"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auVCjBOfWxKE"
      },
      "source": [
        "### Instruction Finetuning with Input-Output Only Dataset\n",
        "\n",
        "In this section, we will use another instruction fine-tuning dataset that contains only input and output pairs, unlike the Alpaca dataset which includes instructions as well. This dataset will help in further refining the model's ability to generate accurate and contextually relevant responses based solely on input prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6h4KiN7fWxKE"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"adithyasean/singlish-instruction-finetune\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4hLr1CfDWxKE"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "### input:\n",
        "{}\n",
        "\n",
        "### output:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token  # Must add EOS_TOKEN\n",
        "\n",
        "def formatting_prompts_func(conversations):\n",
        "    texts = []\n",
        "    inputs = conversations[\"question_prompt\"]\n",
        "    outputs = conversations[\"response_prompt\"]\n",
        "    for input_prompt, response_prompt in zip(inputs, outputs):\n",
        "        # Format the prompt and response using the template\n",
        "        text = prompt.format(input_prompt, response_prompt) + EOS_TOKEN\n",
        "        texts.append(text)\n",
        "    return {\"text\": texts}\n",
        "\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JUoxOt2MWxKE"
      },
      "outputs": [],
      "source": [
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kh7QObIgWxKE"
      },
      "outputs": [],
      "source": [
        "trainer = UnslothTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "\n",
        "    args = UnslothTrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 8,\n",
        "\n",
        "        # Use warmup_ratio and num_train_epochs for longer runs!\n",
        "        max_steps = 120,\n",
        "        warmup_steps = 10,\n",
        "        # warmup_ratio = 0.1,\n",
        "        # num_train_epochs = 1,\n",
        "\n",
        "        # Select a 2 to 10x smaller learning rate for the embedding matrices!\n",
        "        learning_rate = 5e-5,\n",
        "        embedding_learning_rate = 1e-5,\n",
        "\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.00,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "guJ4fTgsWxKE"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIIFmxYeWxKE"
      },
      "source": [
        "### View Resources Usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME086x2tWxKF"
      },
      "outputs": [],
      "source": [
        "# load the last saved model if it is currently not loaded\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"username/model_name\",\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrSvZObor0lY"
      },
      "source": [
        "Using `TextStreamer` for continuous inference\n",
        "\n",
        "*   List item\n",
        "*   List item\n",
        "\n",
        "the generation token by token, instead of waiting the whole time!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2pEuRb1r2Vg"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"\n",
        "### input:\n",
        "{}\n",
        "\n",
        "### output:\n",
        "{}\"\"\"\n",
        "\n",
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    prompt.format(\n",
        "        \"pahadili karanna, bankuwak yanu kumakda?\",\n",
        "        \"\"\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(\"adithyasean/Llama-3.2-3B-Singlish-1.0-4bit\", private = True) # Online saving\n",
        "tokenizer.push_to_hub(\"adithyasean/Llama-3.2-3B-Singlish-1.0-4bit\", private = True) # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Merge to 16bit\n",
        "model.push_to_hub_merged(\"adithyasean/Llama-Singlish-1.0-8B-16bit\", tokenizer, save_method = \"merged_16bit\", token = True, private=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge to 4bit\n",
        "model.push_to_hub_merged(\"adithyasean/Llama-Singlish-1.0-8B-4bit\", tokenizer, save_method = \"merged_4bit\", token = True, private=True)"
      ],
      "metadata": {
        "id": "tAxnSjnhxWs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "### GGUF / llama.cpp Conversion\n",
        "We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on unsloth [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "model.push_to_hub_gguf(\"adithyasean/Llama-Singlish-1.0-8B-Q8-0\", tokenizer, token = True, private=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to 16bit GGUF\n",
        "model.push_to_hub_gguf(\"adithyasean/Llama-Singlish-1.0-8B-f16\", tokenizer, quantization_method = \"f16\", token = True, private=True)"
      ],
      "metadata": {
        "id": "t-etW-Lxxi0d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to q4_k_m GGUF\n",
        "model.push_to_hub_gguf(\"adithyasean/Llama-Singlish-1.0-8B-q4-k-m\", tokenizer, quantization_method = \"q4_k_m\", token = True, private=True)"
      ],
      "metadata": {
        "id": "_aK3LUBmxhst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save to q5_k_m GGUF\n",
        "model.push_to_hub_gguf(\"adithyasean/Llama-Singlish-1.0-8B-q5-k-m\", tokenizer, quantization_method = \"q5_k_m\", token = True, private=True)"
      ],
      "metadata": {
        "id": "vvhboMnZxniW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Disconnect from the Runtime"
      ],
      "metadata": {
        "id": "khE9T0nKzwKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "96Vprt-wz9oI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "cell_execution_strategy": "setup",
      "gpuType": "T4",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "3TvS4TJOWxKA",
        "uIIFmxYeWxKE",
        "ekOmTR1hSNcr",
        "f422JgM9sdVT",
        "TCv4vXHd61i7"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}