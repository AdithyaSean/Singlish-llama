{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries and modules\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from peft import get_peft_model, LoraConfig\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    " \n",
    "# Load the dataset\n",
    "dataset = load_dataset('json', data_files={\n",
    "    'train': '/home/rox/singlish-chatbot/datasets/singlish/singlish_training_dataset2.jsonl',\n",
    "    'validation': '/home/rox/singlish-chatbot/datasets/singlish/singlish_validation_dataset2.jsonl'\n",
    "})\n",
    " \n",
    "# Check if CUDA is available and set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Clear CUDA cache\n",
    "if (device == \"cuda\"):\n",
    "    torch.cuda.empty_cache()\n",
    " \n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/home/rox/llama-singlish/\")\n",
    " \n",
    "# Set the padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    " \n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    # Use \"longest\" to dynamically pad to the longest sequence in each batch\n",
    "    return tokenizer(examples[\"text\"], padding=\"longest\", truncation=True)\n",
    " \n",
    "# Apply tokenization\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    " \n",
    "# Load the model\n",
    "model_name = \"/home/rox/llama-singlish/\"\n",
    " \n",
    "# Configure quantization\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True  # Use 16-bit quantization\n",
    ")\n",
    " \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quant_config)\n",
    " \n",
    "# Configure the PEFT model\n",
    "peft_config = LoraConfig(\n",
    "    r=8,  # Rank of the low-rank adaptation\n",
    "    lora_alpha=32,  # Scaling factor for the low-rank adaptation\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Make sure these are valid for LLaMA 3.1\n",
    "    lora_dropout=0.1,  # Dropout rate for LoRA\n",
    "    bias=\"none\"\n",
    ")\n",
    "peft_model = get_peft_model(model=model, peft_config=peft_config).to(device)\n",
    " \n",
    "# Trainer configuration\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./output\",\n",
    "        num_train_epochs=10,\n",
    "        per_device_train_batch_size=4,  # Reduce batch size\n",
    "        per_device_eval_batch_size=4,  # Reduce batch size\n",
    "        gradient_accumulation_steps=2,  # Accumulate gradients over 2 steps\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=\"./logs\",\n",
    "        remove_unused_columns=False,\n",
    "        fp16=True,  # Use mixed precision training\n",
    "        dataloader_num_workers=2,  # Number of subprocesses to use for data loading\n",
    "        torch_compile=True,  # Enable TorchScript compilation\n",
    "    ),\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    ")\n",
    " \n",
    "# Start training\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
